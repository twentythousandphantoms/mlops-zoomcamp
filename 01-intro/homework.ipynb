{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78575816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1. Downloading the data\n",
    "# We'll use the same NYC taxi dataset, but instead of \"Green Taxi Trip Records\", we'll use \"Yellow Taxi Trip Records\".\n",
    "# Download the data for January and February 2024.\n",
    "# Read the data for January. How many columns are there?\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet')\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5afd61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.851053592192876"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2. Computing duration\n",
    "# Now let's compute the duration variable. It should contain the duration of a ride in minutes.\n",
    "# What's the standard deviation of the trips duration in January?\n",
    "\n",
    "df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "# df.duration.mean()\n",
    "df.duration.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf30b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.78326020432945"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. Dropping outliers\n",
    "# Next, we need to check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "# What fraction of the records left after you dropped the outliers?\n",
    "\n",
    "len(df[(df.duration >= 1) & (df.duration <= 60)]) / len(df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7dcb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.duration >= 1) & (df.duration <= 60)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fea31fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix size: (2898906, 518)\n"
     ]
    }
   ],
   "source": [
    "# Q4. One-hot encoding\n",
    "# Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "# Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "# Fit a dictionary vectorizer\n",
    "# Get a feature matrix from it\n",
    "# What's the dimensionality of this matrix (number of columns)?\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "target_columns = ['PULocationID', 'DOLocationID']\n",
    "df[target_columns] = df[target_columns].astype(str)  # Преобразование в строки\n",
    "data_dicts = df[target_columns].to_dict(orient='records')  # Преобразование в список словарей\n",
    "\n",
    "vectorizer = DictVectorizer()  # Создаём объект DictVectorizer\n",
    "feature_matrix = vectorizer.fit_transform(data_dicts)  # Преобразуем данные в матрицу\n",
    "\n",
    "# Вывод размера матрицы признаков\n",
    "print(f'Feature matrix size: {feature_matrix.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2d93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 7.946173359625103\n"
     ]
    }
   ],
   "source": [
    "# Q5. Training a model\n",
    "# Now let's use the feature matrix from the previous step to train a model.\n",
    "# Train a plain linear regression model with default parameters, where duration is the response variable\n",
    "# Calculate the RMSE of the model on the training data\n",
    "# What's the RMSE on train?\n",
    "\n",
    "# Define the target variable (the column in the DataFrame containing the labels)\n",
    "target_column = 'duration'\n",
    "labels_train = df[target_column].values  # Extract target values as a NumPy array\n",
    "\n",
    "# Initialize the linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model using the training data (feature matrix and target labels)\n",
    "linear_model.fit(feature_matrix, labels_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "predictions_train = linear_model.predict(feature_matrix)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE) between actual and predicted values\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_rmse = mean_squared_error(labels_train, predictions_train, squared=False)\n",
    "print(f'Train RMSE: {train_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7775ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 8.12338297606338\n"
     ]
    }
   ],
   "source": [
    "# Q6. Evaluating the model\n",
    "# Now let's apply this model to the validation dataset (February 2024). \n",
    "# What's the RMSE on validation?\n",
    "\n",
    "# Load and preprocess the validation dataset\n",
    "validation_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet')\n",
    "validation_df['duration'] = validation_df.tpep_dropoff_datetime - validation_df.tpep_pickup_datetime\n",
    "validation_df['duration'] = validation_df.duration.dt.total_seconds() / 60\n",
    "validation_df = validation_df[(validation_df.duration >= 1) & (validation_df.duration <= 60)].copy()\n",
    "\n",
    "# Preprocess validation data\n",
    "target_columns = ['PULocationID', 'DOLocationID']\n",
    "validation_df[target_columns] = validation_df[target_columns].astype(str)\n",
    "validation_data_dicts = validation_df[target_columns].to_dict(orient='records')\n",
    "# Ensure validation data uses the same vectorizer without fitting again\n",
    "validation_feature_matrix = vectorizer.transform(validation_data_dicts)\n",
    "\n",
    "# Extract validation labels\n",
    "target_column = 'duration'\n",
    "validation_labels = validation_df[target_column].values\n",
    "\n",
    "# Make predictions on the validation data\n",
    "# Predict using the already trained linear model\n",
    "validation_pred = linear_model.predict(validation_feature_matrix)\n",
    "# Calculate RMSE for the validation set\n",
    "validation_rmse = mean_squared_error(validation_labels, validation_pred, squared=False)\n",
    "print(f'Validation RMSE: {validation_rmse}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a1794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
